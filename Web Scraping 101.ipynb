{
 "metadata": {
  "name": "",
  "signature": "sha256:2f11bb0158413711267bac832120a17a493d2ee5a0444c491257e66b0fd13926"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use Python's BeautifulSoup package to parse PyCon Au's 2014 videos.\n",
      "\n",
      "The website doesn't have any listed Terms and Conditions or a robot.txt so we should be good to go.\n",
      "\n",
      "We won't use the site's RSS feed as that would be too easy!\n",
      "\n",
      "This tutorial is based on the blog post: http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read HTML into Python String"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pycon_au_2014_url = r'http://pyvideo.org/category/56/pycon-australia-2014'\n",
      "pycon_au_2014_response = requests.get(pycon_au_2014_url)\n",
      "pycon_au_2014_text = pycon_au_2014_response.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup = BeautifulSoup(pycon_au_2014_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Eyeballing the HTML we see that each link to the description of each video is contained in a div with the css class video-summary.\n",
      "\n",
      "Lets populate a list with the URLs to these links."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pycon_au_2014_video_url_list = ['http://pyvideo.org'+video.select('div strong a')[0].get('href') for video in soup.select('.video-summary')]\n",
      "\n",
      "# Notes:\n",
      "# + is defined to concatenate strings in Python\n",
      "# BeautifulSoup's select returns a list so we use [0] to access the first element\n",
      "# get returns the attribute's value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Open each Video Description Page then read in the description and metadata"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "video_descriptions = []\n",
      "\n",
      "for video_url in pycon_au_2014_video_url_list:\n",
      "    video_response = requests.get(video_url)\n",
      "    video_page_text = video_response.text\n",
      "    video_soup = BeautifulSoup(video_page_text)\n",
      "    \n",
      "    video_data = {}\n",
      "    \n",
      "    try:\n",
      "        video_data['Description'] = '\\n'.join([x.text for x in video_soup.select('.section')[1].select('p')])\n",
      "    except IndexError:\n",
      "        video_data['Description'] = None\n",
      "    \n",
      "    video_metadata_categories = [x.text for x in video_soup.select('#sidebar')[0].select('dt')]\n",
      "    video_metadata_values = [x.text.strip() for x in video_soup.select('#sidebar')[0].select('dd')]\n",
      "    for i in range(len(video_metadata_categories)):\n",
      "        video_data[video_metadata_categories[i]] = video_metadata_values[i]\n",
      "        \n",
      "    video_descriptions.append(video_data)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we check if all videos are hosted on Youtube"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all(['http://www.youtube.com/watch?v=' in x['Video origin'] for x in video_descriptions])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We continue our scraping onto Youtube, where in the website's Terms and Services we notice:\n",
      "\n",
      "*You agree not to use or launch any automated system, including without limitation, \"robots,\" \"spiders,\" or \"offline readers,\"\n",
      "**that accesses the Service in a manner that sends more request messages to the YouTube servers in a given period of time than a\n",
      "human can reasonably produce in the same period by using a conventional on-line web browser.**\n",
      "Notwithstanding the foregoing, YouTube grants the operators of public search engines permission to use spiders to copy materials\n",
      "from the site for the sole purpose of and solely to the extent necessary for creating publicly available searchable indices of the materials,\n",
      "but not caches or archives of such materials. YouTube reserves the right to revoke these exceptions either generally or in specific cases.\n",
      "**You agree not to collect or harvest any personally identifiable information, including account names, from the Service, nor to use the communication systems\n",
      "provided by the Service** (e.g., comments, email) for any commercial solicitation purposes.\n",
      "You agree not to solicit, for commercial purposes, any users of the Service with respect to their Content.*\n",
      "\n",
      "We add a reasonable 5 second sleep after each HTTP request, but speed the process up by multiprocessing. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "def get_youtube_data(video_data):\n",
      "    youtube_response = requests.get(video_data['Video origin'])\n",
      "    youtube_page_text = youtube_response.text\n",
      "    youtube_soup = BeautifulSoup(youtube_page_text)\n",
      "    try:\n",
      "        video_data['Youtube views'] = int(youtube_soup.select('.watch-view-count')[0].text.split(' ')[0].replace(',',''))\n",
      "    except IndexError:\n",
      "        if 'This video is unavailable.' in youtube_soup.select('.message')[0].text:\n",
      "            video_data['Youtube views'] = None\n",
      "            video_data['Youtube likes'] = None\n",
      "            video_data['Youtube dislikes'] = None\n",
      "            return video_data\n",
      "    video_data['Youtube likes'] = int(youtube_soup.select('#watch-like .yt-uix-button-content')[0].text.replace(',',''))\n",
      "    video_data['Youtube dislikes'] = int(youtube_soup.select('#watch-dislike .yt-uix-button-content')[0].text.replace(',',''))\n",
      "    return video_data\n",
      "\n",
      "video_descriptions = list(map(get_youtube_data, video_descriptions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above code demonstrates how not using avaliable APIs can be a bad idea. We have very poor error handling.\n",
      "\n",
      "We now save the file as JSON"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "with open(r'./Assets/pycon_au_2014_video_data.json', 'w') as outfile:\n",
      "    json.dump(video_descriptions, outfile)\n",
      "\n",
      "# Code to test that loading works\n",
      "#with open(r'./Assets/pycon_au_2014_video_data.json', 'r') as infile:\n",
      "#    json.load(infile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}